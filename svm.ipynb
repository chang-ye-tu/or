{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\newcommand{\\ds}{\\displaystyle}$ SVM Classifier with $\\ds\\ell_1$-Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we use CVXPY to train a SVM classifier with $\\ds\\ell_1$-regularization $\\newcommand{\\vw}{\\mathbf{w}}$ $\\newcommand{\\vx}{\\mathbf{x}}$ $\\newcommand{\\ds}{\\displaystyle}$\n",
    "- We are given data $\\ds(\\vx_i,\\,y_i)$, $i=1,\\ldots, m$ \n",
    "- The $\\vx_i\\in \\mathbb{R}^n$ are feature vectors, while the $y_i\\in\\{\\pm 1\\}$ are associated boolean outcomes\n",
    "- Goal: To construct a good linear classifier $\\ds\\widehat{y} = {\\rm sgn}(\\vw\\cdot\\vx + b)$\n",
    "- Find the parameters $\\vw$, $b$ by minimizing the convex function\n",
    "\\begin{align*}\n",
    "  f(\\vw, b) = \\frac{1}{m}\\sum_i \\big(1 - y_i (\\vw\\cdot\\vx_i + b) \\big)_+ + \\lambda\\,\\|\\vw\\|_1\n",
    "\\end{align*}\n",
    "- 1st term is the average hinge loss; 2nd term shrinks the coefficients in $\\vw$ and encourages sparsity\n",
    "- $\\lambda \\geqslant 0$ is a regularization parameter\n",
    "- Minimizing $f(\\vw, b)$ simultaneously selects features and fits the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- Generate data with $n=20$ features by randomly choosing $\\vx_i$ and a sparse $\\vw_{\\mathrm{true}} \\in \\mathbb{R}^n$\n",
    "- Set $\\ds y_i = {\\rm sgn}(\\vw_{\\mathrm{true}}\\cdot\\vx_i +b_{\\mathrm{true}} - z_i)$ where the $z_i$ are i.i.d. normal random variables\n",
    "- Divide the data into training and test sets with $m=1000$ examples each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for SVM classifier with L1 regularization.\n",
    "import numpy as np\n",
    "\n",
    "n = 20\n",
    "m = 1000\n",
    "TEST = m\n",
    "DENSITY = 0.2\n",
    "\n",
    "np.random.seed(1)\n",
    "w_true = np.random.randn(n, 1)\n",
    "idxs = np.random.choice(range(n), int((1 - DENSITY) * n), replace=False)\n",
    "for idx in idxs:\n",
    "    w_true[idx] = 0\n",
    "offset = 0\n",
    "sigma = 45\n",
    "X = np.random.normal(0, 5, size=(m, n))\n",
    "Y = np.sign(X.dot(w_true) + offset + np.random.normal(0, sigma, size=(m, 1)))\n",
    "X_test = np.random.normal(0, 5, size=(TEST, n))\n",
    "Y_test = np.sign(X_test.dot(w_true) + offset + np.random.normal(0, sigma, size=(TEST, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Solve the optimization problem for a range of $\\lambda$ to compute a trade-off curve\n",
    "- Plot the train and test error over the trade-off curve\n",
    "- A reasonable choice of $\\lambda$ is the value that minimizes the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Form SVM with L1 regularization problem.\n",
    "import cvxpy as cp\n",
    "\n",
    "w = cp.Variable((n, 1))\n",
    "b = cp.Variable()\n",
    "loss = cp.sum(cp.pos(1 - cp.multiply(Y, X @ w + b)))\n",
    "reg = cp.norm(w, 1)\n",
    "lambd = cp.Parameter(nonneg=True)\n",
    "prob = cp.Problem(cp.Minimize(loss / m + lambd * reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute a trade-off curve and record train and test error.\n",
    "TRIALS = 100\n",
    "train_error = np.zeros(TRIALS)\n",
    "test_error = np.zeros(TRIALS)\n",
    "lambda_vals = np.logspace(-2, 0, TRIALS)\n",
    "w_vals = []\n",
    "for i in range(TRIALS):\n",
    "    lambd.value = lambda_vals[i]\n",
    "    prob.solve()\n",
    "    train_error[i] = (np.sign(X.dot(w_true) + offset) != np.sign(X.dot(w.value) + b.value)).sum() / m\n",
    "    test_error[i] = (np.sign(X_test.dot(w_true) + offset) != np.sign(X_test.dot(w.value) + b.value)).sum() / TEST\n",
    "    w_vals.append(w.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the train and test error over the trade-off curve.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.plot(lambda_vals, train_error, label=\"Train error\")\n",
    "plt.plot(lambda_vals, test_error, label=\"Test error\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Plot the regularization path --- the $\\vw_i$ versus $\\lambda$\n",
    "- $\\vw_i$ do not necessarily decrease monotonically as $\\lambda$ increases\n",
    "- 4 features remain non-zero longer for larger $\\lambda$; these features are the most important \n",
    "- In fact $\\vw_{\\mathrm{true}}$ had 4 non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the regularization path for w.\n",
    "for i in range(n):\n",
    "    plt.plot(lambda_vals, [wi[i, 0] for wi in w_vals])\n",
    "plt.xlabel(r\"$\\lambda$\", fontsize=16)\n",
    "plt.xscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
