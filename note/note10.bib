%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Deep Ray at 2021-10-27 15:53:07 -0700 


%% Saved with string encoding Unicode (UTF-8) 

@misc{li2020fourier,
  doi = {10.48550/ARXIV.2010.08895},
  
  howpublished ="\url{https://arxiv.org/abs/2010.08895}",
  
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  
  keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Fourier Neural Operator for Parametric Partial Differential Equations},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{patel2022variationally,
  doi = {10.48550/ARXIV.2209.12871},
  
  howpublished="\url{https://arxiv.org/abs/2209.12871}",
  
  author = {Patel, Dhruv and Ray, Deep and Abdelmalik, Michael R. A. and Hughes, Thomas J. R. and Oberai, Assad A.},
  
  keywords = {Numerical Analysis (math.NA), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences, 65N99, 35J20},
  
  title = {Variationally Mimetic Operator Networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@article{patel2022,
	author = {Dhruv V. Patel and Deep Ray and Assad A. Oberai},
	doi = {https://doi.org/10.1016/j.cma.2022.115428},
	issn = {0045-7825},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	keywords = {Bayesian inference, Inverse problems, Uncertainty quantification (UQ), Model order reduction, Markov Chain Monte Carlo (MCMC), Elastography},
	pages = {115428},
	title = {Solution of physics-based Bayesian inverse problems with deep generative priors},
	url = {https://www.sciencedirect.com/science/article/pii/S004578252200473X},
	volume = {400},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S004578252200473X},
	bdsk-url-2 = {https://doi.org/10.1016/j.cma.2022.115428}}


@article{pi_deeponet,
author = {Sifan Wang  and Hanwen Wang  and Paris Perdikaris },
title = {Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
journal = {Science Advances},
volume = {7},
number = {40},
year = {2021},
doi = {10.1126/sciadv.abi8605},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.abi8605},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abi8605}}

@article{sid_deeponet,
    author = {Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George E},
    title = "{Error estimates for DeepONets: a deep learning framework in infinite dimensions}",
    journal = {Transactions of Mathematics and Its Applications},
    volume = {6},
    number = {1},
    year = {2022},
    month = {03},
    issn = {2398-4945},
    doi = {10.1093/imatrm/tnac001},
    url = {https://doi.org/10.1093/imatrm/tnac001},
    eprint = {https://academic.oup.com/imatrm/article-pdf/6/1/tnac001/42785544/tnac001.pdf},
}


@article{chen95,
	author = {Tianping Chen and Hong Chen},
	date-added = {2021-10-27 15:50:16 -0700},
	date-modified = {2021-10-27 15:50:24 -0700},
	doi = {10.1109/72.392253},
	journal = {IEEE Transactions on Neural Networks},
	number = {4},
	pages = {911-917},
	title = {Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
	volume = {6},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/72.392253}}

@article{deeponet,
	abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
	author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
	date = {2021/03/01},
	date-added = {2021-10-27 15:49:04 -0700},
	date-modified = {2021-10-27 15:49:18 -0700},
	doi = {10.1038/s42256-021-00302-5},
	id = {Lu2021},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {3},
	pages = {218--229},
	title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
	url = {https://doi.org/10.1038/s42256-021-00302-5},
	volume = {3},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-021-00302-5}}


@inproceedings{ronneberger2015unet,
	address = {Cham},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	isbn = {978-3-319-24574-4},
	pages = {234--241},
	publisher = {Springer International Publishing},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	year = {2015}}


@article{mishrapinns,
    author = {Mishra, Siddhartha and Molinaro, Roberto},
    title = "{Estimates on the generalization error of physics-informed neural networks for approximating PDEs}",
    journal = {IMA Journal of Numerical Analysis},
    year = {2022},
    month = {01},
    issn = {0272-4979},
    doi = {10.1093/imanum/drab093},
    url = {https://doi.org/10.1093/imanum/drab093},
    eprint = {https://academic.oup.com/imajna/advance-article-pdf/doi/10.1093/imanum/drab093/42198350/drab093.pdf},
}


@article{PINNs_Lagaris,
	author = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
	date-added = {2021-09-30 23:37:07 -0700},
	date-modified = {2021-09-30 23:37:20 -0700},
	doi = {10.1109/72.870037},
	journal = {IEEE Transactions on Neural Networks},
	number = {5},
	pages = {1041-1049},
	title = {Neural-network methods for boundary value problems with irregular boundaries},
	volume = {11},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/72.870037}}


@misc{chen2019neural,
  doi = {10.48550/ARXIV.1806.07366},
  
  howpublished = "\url{https://arxiv.org/abs/1806.07366}",
  
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Ordinary Differential Equations},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
 

@misc{kingma2017adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  howpublished = "\url{https://arxiv.org/abs/1412.6980v9}",
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{he2015deep,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

@inproceedings{WuSGD,
	author = {Wu, Lei and Ma, Chao and E, Weinan},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2021-09-17 20:50:13 -0700},
	date-modified = {2021-09-17 20:50:19 -0700},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective},
	url = {https://proceedings.neurips.cc/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf}}

@article{nemirovski90,
	author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
	date-added = {2021-09-17 12:41:01 -0700},
	date-modified = {2021-09-17 12:41:19 -0700},
	doi = {10.1137/070704277},
	eprint = {https://doi.org/10.1137/070704277},
	journal = {SIAM Journal on Optimization},
	number = {4},
	pages = {1574-1609},
	title = {Robust Stochastic Approximation Approach to Stochastic Programming},
	url = {https://doi.org/10.1137/070704277},
	volume = {19},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1137/070704277}}

@article{raissi2019,
	abstract = {We introduce physics-informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge--Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction--diffusion systems, and the propagation of nonlinear shallow-water waves.},
	author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
	date-added = {2021-08-31 19:41:37 -0700},
	date-modified = {2021-08-31 19:41:50 -0700},
	doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge--Kutta methods, Nonlinear dynamics},
	pages = {686-707},
	title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	volume = {378},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcp.2018.10.045}}

@inproceedings{Kidger2020,
	abstract = { The classical Universal Approximation Theorem holds for neural networks of arbitrary width and bounded depth. Here we consider the natural `dual' scenario for networks of bounded width and arbitrary depth. Precisely, let $n$ be the number of inputs neurons, $m$ be the number of output neurons, and let $\rho$ be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then we show that the class of neural networks of arbitrary depth, width $n + m + 2$, and activation function $\rho$, is dense in $C(K; \mathbb{R}^m)$ for $K \subseteq \mathbb{R}^n$ with $K$ compact. This covers every activation function possible to use in practice, and also includes polynomial activation functions, which is unlike the classical version of the theorem, and provides a qualitative difference between deep narrow networks and shallow wide networks. We then consider several extensions of this result. In particular we consider nowhere differentiable activation functions, density in noncompact domains with respect to the $L^p$-norm, and how the width may be reduced to just $n + m + 1$ for `most' activation functions.},
	author = {Kidger, Patrick and Lyons, Terry},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	date-added = {2021-08-30 16:10:01 -0700},
	date-modified = {2021-08-30 16:10:12 -0700},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = {09--12 Jul},
	pages = {2306--2327},
	pdf = {http://proceedings.mlr.press/v125/kidger20a/kidger20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{Universal Approximation with Deep Narrow Networks}},
	url = {https://proceedings.mlr.press/v125/kidger20a.html},
	volume = {125},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v125/kidger20a.html}}

@article{Pinkus1999,
	author = {Pinkus, Allan},
	date-added = {2021-08-30 15:21:07 -0700},
	date-modified = {2021-08-30 15:21:15 -0700},
	doi = {10.1017/S0962492900002919},
	journal = {Acta Numerica},
	pages = {143--195},
	publisher = {Cambridge University Press},
	title = {Approximation theory of the MLP model in neural networks},
	volume = {8},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1017/S0962492900002919}}

@misc{Yarotsky2021,
  doi = {10.48550/ARXIV.1906.09477},
  
  howpublished="\url{https://arxiv.org/abs/1906.09477}",
  
  author = {Yarotsky, Dmitry and Zhevnerchuk, Anton},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The phase diagram of approximation rates for deep neural networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Mass2013,
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
	booktitle = {Proc. ICML},
	date-added = {2021-08-25 11:16:58 -0700},
	date-modified = {2021-08-25 11:17:05 -0700},
	number = {1},
	title = {Rectifier nonlinearities improve neural network acoustic models},
	volume = {30},
	year = {2013}}

@misc{siren2020,
  doi = {10.48550/ARXIV.2006.09661},
  
  howpublished="\url{https://arxiv.org/abs/2006.09661}",
  
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Implicit Neural Representations with Periodic Activation Functions},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{keller1976,
	author = {Joseph B. Keller},
	doi = {10.1080/00029890.1976.11994053},
	eprint = {https://doi.org/10.1080/00029890.1976.11994053},
	journal = {The American Mathematical Monthly},
	number = {2},
	pages = {107-118},
	publisher = {Taylor & Francis},
	title = {Inverse Problems},
	url = {https://doi.org/10.1080/00029890.1976.11994053},
	volume = {83},
	year = {1976},
	bdsk-url-1 = {https://doi.org/10.1080/00029890.1976.11994053}}

@article{hadamard1902problemes,
	author = {Hadamard, Jacques},
	journal = {Princeton university bulletin},
	pages = {49--52},
	title = {Sur les probl{\`e}mes aux d{\'e}riv{\'e}es partielles et leur signification physique},
	year = {1902}}

@article{Chrysos2020,
	abstract = {Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli).},
	author = {Chrysos, Grigorios G. and Kossaifi, Jean and Zafeiriou, Stefanos},
	da = {2020/11/01},
	date-added = {2021-01-22 14:53:58 -0600},
	date-modified = {2021-01-22 14:54:12 -0600},
	doi = {10.1007/s11263-020-01348-5},
	id = {Chrysos2020},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {10},
	pages = {2665--2683},
	title = {RoCGAN: Robust Conditional GAN},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11263-020-01348-5},
	volume = {128},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s11263-020-01348-5}}

@inproceedings{augcgan2018,
	abstract = {Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.},
	address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	author = {Almahairi, Amjad and Rajeshwar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	date-added = {2021-01-22 14:49:06 -0600},
	date-modified = {2021-01-22 14:49:25 -0600},
	editor = {Jennifer Dy and Andreas Krause},
	month = {10--15 Jul},
	pages = {195--204},
	pdf = {http://proceedings.mlr.press/v80/almahairi18a/almahairi18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Augmented {C}ycle{GAN}: Learning Many-to-Many Mappings from Unpaired Data},
	url = {http://proceedings.mlr.press/v80/almahairi18a.html},
	volume = {80},
	year = {2018},
	bdsk-url-1 = {http://proceedings.mlr.press/v80/almahairi18a.html}}

@inproceedings{lucic2018,
	abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].},
	address = {Red Hook, NY, USA},
	author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	date-added = {2021-01-22 14:47:27 -0600},
	date-modified = {2021-01-22 14:47:48 -0600},
	location = {Montr\'{e}al, Canada},
	numpages = {10},
	pages = {698--707},
	publisher = {Curran Associates Inc.},
	series = {NIPS'18},
	title = {Are GANs Created Equal? A Large-Scale Study},
	year = {2018}}

@article{Miyato2018SpectralNF,
	author = {Takeru Miyato and T. Kataoka and Masanori Koyama and Y. Yoshida},
	date-added = {2021-01-03 21:37:09 -0600},
	date-modified = {2021-01-03 21:37:09 -0600},
	journal = {ArXiv},
	title = {Spectral Normalization for Generative Adversarial Networks},
	volume = {abs/1802.05957},
	year = {2018}}

@book{villani2008optimal,
	author = {Villani, C.},
	date-added = {2021-01-02 15:04:31 -0600},
	date-modified = {2021-01-02 15:04:31 -0600},
	isbn = {9783540710509},
	lccn = {2008932183},
	publisher = {Springer Berlin Heidelberg},
	series = {Grundlehren der mathematischen Wissenschaften},
	title = {Optimal Transport: Old and New},
	url = {https://books.google.com/books?id=hV8o5R7\_5tkC},
	year = {2008},
	bdsk-url-1 = {https://books.google.com/books?id=hV8o5R7%5C_5tkC}}

@inproceedings{arjovsky2017wasserstein_proc,
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	address = {International Convention Centre, Sydney, Australia},
	author = {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2021-01-02 15:00:29 -0600},
	date-modified = {2021-01-02 15:01:07 -0600},
	editor = {Doina Precup and Yee Whye Teh},
	month = {06--11 Aug},
	pages = {214--223},
	pdf = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{W}asserstein Generative Adversarial Networks},
	url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {http://proceedings.mlr.press/v70/arjovsky17a.html}}

@article{dashti2016bayesian,
	author = {Dashti, Masoumeh and Stuart, Andrew M},
	journal = {Handbook of Uncertainty Quantification},
	pages = {1--118},
	publisher = {Springer},
	title = {The Bayesian approach to inverse problems},
	year = {2016}}

@article{Pavan2012,
	abstract = {The strain image contrast of some in vivo breast lesions changes with increasing applied load. This change is attributed to differences in the nonlinear elastic properties of the constituent tissues suggesting some potential to help classify breast diseases by their nonlinear elastic properties. A phantom with inclusions and long-term stability is desired to serve as a test bed for nonlinear elasticity imaging method development, testing, etc. This study reports a phantom designed to investigate nonlinear elastic properties with ultrasound elastographic techniques. The phantom contains four spherical inclusions and was manufactured from a mixture of gelatin, agar and oil. The phantom background and each of the inclusions have distinct Young's modulus and nonlinear mechanical behavior. This phantom was subjected to large deformations (up to 20{\%}) while scanning with ultrasound, and changes in strain image contrast and contrast-to-noise ratio between inclusion and background, as a function of applied deformation, were investigated. The changes in contrast over a large deformation range predicted by the finite element analysis (FEA) were consistent with those experimentally observed. Therefore, the paper reports a procedure for making phantoms with predictable nonlinear behavior, based on independent measurements of the constituent materials, and shows that the resulting strain images (e.g., strain contrast) agree with that predicted with nonlinear FEA. {\textcopyright} 2012 Institute of Physics and Engineering in Medicine.},
	author = {Pavan, Theo Z. and Madsen, Ernest L. and Frank, Gary R. and Jiang, Jingfeng and Carneiro, Antonio A.O. and Hall, Timothy J.},
	doi = {10.1088/0031-9155/57/15/4787},
	issn = {00319155},
	journal = {Physics in Medicine and Biology},
	keywords = {elastography,finite element methods,nonlinear hyperelasticity,phantoms},
	month = {aug},
	number = {15},
	pages = {4787--4804},
	pmid = {22772074},
	publisher = {NIH Public Access},
	title = {{A nonlinear elasticity phantom containing spherical inclusions}},
	url = {/pmc/articles/PMC3413382/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3413382/},
	volume = {57},
	year = {2012},
	bdsk-url-1 = {/pmc/articles/PMC3413382/?report=abstract%20https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3413382/},
	bdsk-url-2 = {https://doi.org/10.1088/0031-9155/57/15/4787}}

@book{Polpo2018,
	address = {Cham},
	doi = {10.1007/978-3-319-91143-4},
	editor = {Polpo, Adriano and Stern, Julio and Louzada, Francisco and Izbicki, Rafael and Takada, Hellinton},
	isbn = {978-3-319-91142-7},
	publisher = {Springer International Publishing},
	series = {Springer Proceedings in Mathematics {\&} Statistics},
	title = {{Bayesian Inference and Maximum Entropy Methods in Science and Engineering}},
	url = {http://link.springer.com/10.1007/978-3-319-91143-4},
	volume = {239},
	year = {2018},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-319-91143-4},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-91143-4}}

@book{tarantola2005inverse,
	author = {Tarantola, Albert},
	publisher = {siam},
	title = {Inverse problem theory and methods for model parameter estimation},
	volume = {89},
	year = {2005}}

@article{Gouveia1997,
	author = {Gouveia, Wences P and Scales, John A},
	doi = {10.1088/0266-5611/13/2/009},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {apr},
	number = {2},
	pages = {323--349},
	publisher = {IOP Publishing},
	title = {{Resolution of seismic waveform inversion: Bayes versus Occam}},
	url = {http://stacks.iop.org/0266-5611/13/i=2/a=009?key=crossref.c3e937d46f2de4adfe9aa2de3c226f3e},
	volume = {13},
	year = {1997},
	bdsk-url-1 = {http://stacks.iop.org/0266-5611/13/i=2/a=009?key=crossref.c3e937d46f2de4adfe9aa2de3c226f3e},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/13/2/009}}

@article{Malinverno2002,
	author = {Malinverno, A.},
	doi = {10.1046/j.1365-246X.2002.01847.x},
	issn = {0956-540X},
	journal = {Geophysical Journal International},
	keywords = {Bayesian inversion,MCMC,Markov chain Monte Carlo,geophysical inversion,resistivity},
	month = {dec},
	number = {3},
	pages = {675--688},
	publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
	title = {{Parsimonious Bayesian Markov chain Monte Carlo inversion in a nonlinear geophysical problem}},
	url = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.2002.01847.x},
	volume = {151},
	year = {2002},
	bdsk-url-1 = {https://academic.oup.com/gji/article-lookup/doi/10.1046/j.1365-246X.2002.01847.x},
	bdsk-url-2 = {https://doi.org/10.1046/j.1365-246X.2002.01847.x}}

@article{Martin2012,
	abstract = {We address the solution of large-scale statistical inverse problems in the framework of Bayesian inference. The Markov chain Monte Carlo (MCMC) method is the most popular approach for sampling the ...},
	author = {Martin, James and Wilcox, Lucas C. and Burstedde, Carsten and Ghattas, Omar},
	doi = {10.1137/110845598},
	issn = {1064-8275},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {35Q62,35Q86,35Q93,49M15,65C40,65C60,65M32,74J20,74J25,Langevin dynamics,MCMC,Stochastic Newton,inverse problems,low-rank Hessian,uncertainty quantification},
	month = {jan},
	number = {3},
	pages = {A1460--A1487},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{A Stochastic Newton MCMC Method for Large-Scale Statistical Inverse Problems with Application to Seismic Inversion}},
	url = {http://epubs.siam.org/doi/10.1137/110845598},
	volume = {34},
	year = {2012},
	bdsk-url-1 = {http://epubs.siam.org/doi/10.1137/110845598},
	bdsk-url-2 = {https://doi.org/10.1137/110845598}}

@article{Isaac2015,
	abstract = {The majority of research on efficient and scalable algorithms in computational science and engineering has focused on the forward problem: given parameter inputs, solve the governing equations to determine output quantities of interest. In contrast, here we consider the broader question: given a (large-scale) model containing uncertain parameters, (possibly) noisy observational data, and a prediction quantity of interest, how do we construct efficient and scalable algorithms to (1) infer the model parameters from the data (the deterministic inverse problem), (2) quantify the uncertainty in the inferred parameters (the Bayesian inference problem), and (3) propagate the resulting uncertain parameters through the model to issue predictions with quantified uncertainties (the forward uncertainty propagation problem)? We present efficient and scalable algorithms for this end-to-end, data-to-prediction process under the Gaussian approximation and in the context of modeling the flow of the Antarctic ice sheet and its effect on loss of grounded ice to the ocean. The ice is modeled as a viscous, incompressible, creeping, shear-thinning fluid. The observational data come from satellite measurements of surface ice flow velocity, and the uncertain parameter field to be inferred is the basal sliding parameter, represented by a heterogeneous coefficient in a Robin boundary condition at the base of the ice sheet. The prediction quantity of interest is the present-day ice mass flux from the Antarctic continent to the ocean. We show that the work required for executing this data-to-prediction process---measured in number of forward (and adjoint) ice sheet model solves---is independent of the state dimension, parameter dimension, data dimension, and the number of processor cores. The key to achieving this dimension independence is to exploit the fact that, despite their large size, the observational data typically provide only sparse information on model parameters. This property can be exploited to construct a low rank approximation of the linearized parameter-to-observable map via randomized SVD methods and adjoint-based actions of Hessians of the data misfit functional.},
	author = {Isaac, Tobin and Petra, Noemi and Stadler, Georg and Ghattas, Omar},
	doi = {10.1016/J.JCP.2015.04.047},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	month = {sep},
	pages = {348--368},
	publisher = {Academic Press},
	title = {{Scalable and efficient algorithms for the propagation of uncertainty from data through inference to prediction for large-scale problems, with application to flow of the Antarctic ice sheet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999115003046},
	volume = {296},
	year = {2015},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999115003046},
	bdsk-url-2 = {https://doi.org/10.1016/J.JCP.2015.04.047}}

@article{Jackson2004,
	abstract = {Abstract One source of uncertainty for climate model predictions arises from the fact that climate models have been optimized to reproduce observational means. To quantify the uncertainty resulting...},
	author = {Jackson, Charles and Sen, Mrinal K. and Stoffa, Paul L. and Jackson, Charles and Sen, Mrinal K. and Stoffa, Paul L.},
	doi = {10.1175/1520-0442(2004)017<2828:AESBAT>2.0.CO;2},
	issn = {0894-8755},
	journal = {Journal of Climate},
	month = {jul},
	number = {14},
	pages = {2828--2841},
	title = {{An Efficient Stochastic Bayesian Approach to Optimal Parameter and Uncertainty Estimation for Climate Model Predictions}},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0442{\%}282004{\%}29017{\%}3C2828{\%}3AAESBAT{\%}3E2.0.CO{\%}3B2},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1175/1520-0442(2004)017%3C2828:AESBAT%3E2.0.CO;2}}

@article{Najm2009,
	author = {Najm, H. N. and Debusschere, B. J. and Marzouk, Y. M. and Widmer, S. and {Le Ma{\~{A}}{\textregistered}tre}, O. P.},
	doi = {10.1002/nme.2551},
	issn = {00295981},
	journal = {International Journal for Numerical Methods in Engineering},
	keywords = {chemistry,ignition,multiwavelet,polynomial chaos,uncertainty quantification},
	month = {nov},
	number = {6{\^{a}}7},
	pages = {789--814},
	publisher = {John Wiley {\&} Sons, Ltd},
	title = {{Uncertainty quantification in chemical systems}},
	url = {http://doi.wiley.com/10.1002/nme.2551},
	volume = {80},
	year = {2009},
	bdsk-url-1 = {http://doi.wiley.com/10.1002/nme.2551},
	bdsk-url-2 = {https://doi.org/10.1002/nme.2551}}

@article{Wang_2004,
	abstract = {Stochastic inverse problems in heat conduction with consideration of uncertainties in the measured temperature data, temperature sensor locations and thermophysical properties are addressed using a Bayesian statistical inference method. Both parameter estimation and thermal history reconstruction problems, including boundary heat flux and heat source reconstruction, are studied. Probabilistic specification of the unknown variables is deduced from temperature measurements. Hierarchical Bayesian models are adopted to relax the prior assumptions on the unknowns. The use of a hierarchical Bayesian method for automatic selection of the regularization parameter in the function estimation inverse problem is discussed. In addition, the method explores the length scales in the estimation of thermal variables varying in space and time. Markov chain Monte Carlo (MCMC) simulation is conducted to explore the high dimensional posterior state space. The methodologies presented are general and applicable to a number of data-driven engineering inverse problems.},
	author = {Jingbo Wang and Nicholas Zabaras},
	doi = {10.1088/0266-5611/21/1/012},
	journal = {Inverse Problems},
	month = {dec},
	number = {1},
	pages = {183--206},
	publisher = {{IOP} Publishing},
	title = {Hierarchical Bayesian models for inverse problems in heat conduction},
	url = {https://doi.org/10.1088%2F0266-5611%2F21%2F1%2F012},
	volume = {21},
	year = 2004,
	bdsk-url-1 = {https://doi.org/10.1088%2F0266-5611%2F21%2F1%2F012},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/21/1/012}}

@incollection{Loredo1990,
	address = {Dordrecht},
	author = {Loredo, T. J.},
	booktitle = {Maximum Entropy and Bayesian Methods},
	doi = {10.1007/978-94-009-0683-9_6},
	pages = {81--142},
	publisher = {Springer Netherlands},
	title = {{From Laplace to Supernova SN 1987A: Bayesian Inference in Astrophysics}},
	url = {http://link.springer.com/10.1007/978-94-009-0683-9{\_}6},
	year = {1990},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-94-009-0683-9%7B%5C_%7D6},
	bdsk-url-2 = {https://doi.org/10.1007/978-94-009-0683-9_6}}

@inproceedings{Craig1986InversePI,
	author = {I. Craig and John C. Brown},
	title = {Inverse Problems in Astronomy, A guide to inversion strategies for remotely sensed data},
	year = {1986}}

@article{Lindgren2020,
	abstract = {We consider uncertainty aware compressive sensing when the prior distribution is defined by an invertible generative model. In this problem, we receive a set of low dimensional measurements and we want to generate conditional samples of high dimensional objects conditioned on these measurements. We first show that the conditional sampling problem is hard in general, and thus we consider approximations to the problem. We develop a variational approach to conditional sampling that composes a new generative model with the given generative model. This allows us to utilize the sampling ability of the given generative model to quickly generate samples from the conditional distribution.},
	archiveprefix = {arXiv},
	arxivid = {2002.11743},
	author = {Lindgren, Erik M. and Whang, Jay and Dimakis, Alexandros G.},
	eprint = {2002.11743},
	month = {feb},
	title = {{Conditional Sampling from Invertible Generative Models with Applications to Inverse Problems}},
	url = {http://arxiv.org/abs/2002.11743},
	year = {2020},
	bdsk-url-1 = {http://arxiv.org/abs/2002.11743}}

@article{Zhang2019,
	abstract = {Aleatoric uncertainty is an intrinsic property of ill-posed inverse and imaging problems. Its quantification is vital for assessing the reliability of relevant point estimates. In this paper, we propose an efficient framework for quantifying aleatoric uncertainty for deep residual learning and showcase its significant potential on image restoration. In the framework, we divide the conditional probability modeling for the residual variable into a deterministic homo-dimensional level, a stochastic low-dimensional level and a merging level. The low-dimensionality is especially suitable for sparse correlation between image pixels, enables efficient sampling for high dimensional problems and acts as a regularizer for the distribution. Preliminary numerical experiments show that the proposed method can give not only state-of-the-art point estimates of image restoration but also useful associated uncertainty information.},
	archiveprefix = {arXiv},
	arxivid = {1908.01010},
	author = {Zhang, Chen and Jin, Bangti},
	eprint = {1908.01010},
	month = {aug},
	title = {{Probabilistic Residual Learning for Aleatoric Uncertainty in Image Restoration}},
	url = {http://arxiv.org/abs/1908.01010},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1908.01010}}

@article{Ardizzone2019AnalyzingIP,
	author = {Lynton Ardizzone and Jakob Kruse and Sebastian J. Wirkert and D. Rahner and Eric W. Pellegrini and R. Klessen and L. Maier-Hein and C. Rother and U. K{\"o}the},
	journal = {ArXiv},
	title = {Analyzing Inverse Problems with Invertible Neural Networks},
	volume = {abs/1808.04730},
	year = {2019}}

@article{Xu2021solving,
	abstract = {Inverse problems associated with stochastic models constitute a significant portion of scientific and engineering applications. In such cases the unknown quantities are distributions. The applicability of traditional methods is limited because of their demanding assumptions or prohibitive computational consumption; for example, maximum likelihood methods require closed-form density functions, and Markov Chain Monte Carlo needs a large number of simulations. We propose a new method that estimates the unknown distribution by matching the statistical properties between observed and simulated random processes. We leverage the expressive power of neural networks to approximate the unknown distribution and use a discriminative neural network for computing the statistical discrepancies between the observed and simulated random processes. We demonstrated numerically that the proposed methods can estimate both the model parameters and learn complicated unknown distributions.},
	author = {Xu, Kailai and Darve, Eric},
	doi = {10.1016/j.cma.2021.113976},
	issn = {00457825},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	keywords = {Adversarial training,Automatic differentiation,Neural networks},
	month = {oct},
	pages = {113976},
	publisher = {Elsevier B.V.},
	title = {{Solving inverse problems in stochastic models using deep neural networks and adversarial training}},
	volume = {384},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1016/j.cma.2021.113976}}

@article{Tonolini2019,
	abstract = {Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.},
	archiveprefix = {arXiv},
	arxivid = {1904.06264},
	author = {Tonolini, Francesco and Radford, Jack and Turpin, Alex and Faccio, Daniele and Murray-Smith, Roderick},
	eprint = {1904.06264},
	keywords = {Approximate Inference,Bayesian Inference,Computational Imaging,Inverse Problems},
	month = {apr},
	title = {{Variational Inference for Computational Imaging Inverse Problems}},
	url = {http://arxiv.org/abs/1904.06264},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1904.06264}}

@article{Li2006efficient,
	abstract = {The geostatistical inverse approach of Kitanidis (1995) requires the covariance matrix of a discretized spatial random variable and matrix multiplications of the covariance matrix. The explicit computation of the full covariance matrix is prohibitive for large-scale problems. Discrete spectral methods, which help to reduce computational costs dramatically, are restricted to regular grids. To make the geostatistical inverse approach applicable to unstructured grids, which may be of great interest for practical applications, we use the functional parameterization of the spatial random field by the Karhunen-Lo{\`{e}}ve (KL) expansion, in which the spatial variable is parameterized by weighted base functions that are derived from the covariance function. In the inverse approach, the estimation of the spatial variable becomes estimating the weights of base functions. The prior covariance matrix of the weights is the identity matrix. The base functions are continuous in space and can be discretized in any fashion. For the derivation of the base functions, we embed the domain into a larger unit cell of a periodic domain. Then, the base functions are sinusoidal and can be derived by spectral methods which are independent of the discretization used for forward and inverse modeling. For illustration, we present case studies using synthetic data to estimate hydraulic conductivities on regular and unstructured grids. The results show that we can efficiently obtain a good estimate using the inverse method in conjunction with the KL expansion. We also demonstrate how to estimate structural parameters and to generate conditional realizations in the framework of our method. Copyright 2006 by the American Geophysical Union.},
	author = {Li, Wei and Cirpka, Olaf A.},
	doi = {10.1029/2005WR004668},
	file = {:C$\backslash$:/Users/Harikrishna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Cirpka - 2006 - Efficient geostatistical inverse methods for structured and unstructured grids.pdf:pdf},
	issn = {00431397},
	journal = {Water Resources Research},
	keywords = {Karhunen{\&}hyphen,Lo{\&}egrave,doi:10.1029/2005WR004668,geostatistical inference,http://dx.doi.org/10.1029/2005WR004668,periodic embeddding,ve expansion},
	month = {jun},
	number = {6},
	publisher = {John Wiley {\&} Sons, Ltd},
	title = {{Efficient geostatistical inverse methods for structured and unstructured grids}},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005WR004668 https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005WR004668 https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2005WR004668},
	volume = {42},
	year = {2006},
	bdsk-url-1 = {https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005WR004668%20https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005WR004668%20https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2005WR004668},
	bdsk-url-2 = {https://doi.org/10.1029/2005WR004668}}

@article{AsensioRamos2007,
	abstract = {{\textless}i{\textgreater}Context.{\textless}i/{\textgreater}Inversion techniques are the most powerful methods to obtain information about the thermodynamical and magnetic properties of solar and stellar atmospheres. In the recent years, we have witnessed the development of highly sophisticated inversion codes that are now widely applied to spectro-polarimetric observations. The majority of these inversion codes are based on the optimization of a complicated non-linear merit function. The experience gained has facilitated the recovery of the model that best fits a given observation. However, and except for the recently developed inversion codes based on database search algorithms together with the application of Principal Component Analysis, no reliable and statistically well-defined confidence intervals can be obtained for the parameters inferred from the inversions.{\textless}i{\textgreater}Aims.{\textless}i/{\textgreater}A correct estimation of the confidence intervals for all the parameters that describe the model is mandatory. Additionally, it is fundamental to apply efficient techniques to assess the ability of models to reproduce the observations and to determine to what extent the models have to be refined or can be simplified.{\textless}i{\textgreater}Methods.{\textless}i/{\textgreater}Bayesian techniques are applied to analyze the performance of the model to fit a given observed Stokes vector. The posterior distribution, that takes into account both the information about the priors and the likelihood, is efficiently sampled using a Markov chain Monte Carlo method. For simplicity, we focus on the Milne-Eddington approximate solution of the radiative transfer equation and we only take into account the generation of polarization through the Zeeman effect. However, the method is extremely general and other more complex forward models can be applied, even allowing for the presence of atomic polarization.{\textless}i{\textgreater}Results.{\textless}i/{\textgreater}We illustrate the method with different problems, from academic to more realistic examples. We show that the information provided by the posterior distribution is fundamental to understand and determine the amount of information available in the Stokes profiles in these particular cases.},
	author = {{Asensio Ramos}, A. and {Mart{\'{i}}nez Gonz{\'{a}}lez}, M. J. and Rubi{\~{n}}o-Mart{\'{i}}n, J. A.},
	doi = {10.1051/0004-6361:20078107},
	issn = {0004-6361},
	journal = {Astronomy {\&} Astrophysics},
	keywords = {Sun: atmosphere,Sun: magnetic fields,line: profiles,magnetic fields,polarization},
	month = {dec},
	number = {2},
	pages = {959--970},
	publisher = {EDP Sciences},
	title = {{Bayesian inversion of Stokes profiles}},
	url = {http://www.aanda.org/10.1051/0004-6361:20078107},
	volume = {476},
	year = {2007},
	bdsk-url-1 = {http://www.aanda.org/10.1051/0004-6361:20078107},
	bdsk-url-2 = {https://doi.org/10.1051/0004-6361:20078107}}

@article{Sabin2000,
	author = {Sabin, T J and Bailer-Jones, C A L and Withers, P J},
	doi = {10.1088/0965-0393/8/5/304},
	issn = {0965-0393},
	journal = {Modelling and Simulation in Materials Science and Engineering},
	month = {sep},
	number = {5},
	pages = {687--706},
	publisher = {IOP Publishing},
	title = {{Accelerated learning using Gaussian process models to predict static recrystallization in an Al-Mg alloy}},
	url = {http://stacks.iop.org/0965-0393/8/i=5/a=304?key=crossref.0a7cd40dc84219e2de59f113e710378b},
	volume = {8},
	year = {2000},
	bdsk-url-1 = {http://stacks.iop.org/0965-0393/8/i=5/a=304?key=crossref.0a7cd40dc84219e2de59f113e710378b},
	bdsk-url-2 = {https://doi.org/10.1088/0965-0393/8/5/304}}

@article{Siltanen2003,
	author = {Siltanen, S and Kolehmainen, V and J rvenp, S and Kaipio, J P and Koistinen, P and Lassas, M and Pirttil, J and Somersalo, E},
	doi = {10.1088/0031-9155/48/10/314},
	issn = {0031-9155},
	journal = {Physics in Medicine and Biology},
	month = {may},
	number = {10},
	pages = {1437--1463},
	publisher = {IOP Publishing},
	title = {{Statistical inversion for medical x-ray tomography with few radiographs: I. General theory}},
	url = {http://stacks.iop.org/0031-9155/48/i=10/a=314?key=crossref.5fce2d21d49cf69f7c7b946fb1945c85},
	volume = {48},
	year = {2003},
	bdsk-url-1 = {http://stacks.iop.org/0031-9155/48/i=10/a=314?key=crossref.5fce2d21d49cf69f7c7b946fb1945c85},
	bdsk-url-2 = {https://doi.org/10.1088/0031-9155/48/10/314}}

@article{Kolehmainen2006,
	author = {Kolehmainen, V. and Vanne, A. and Siltanen, S. and Jarvenpaa, S. and Kaipio, J.P. and Lassas, M. and Kalke, M.},
	doi = {10.1109/TMI.2005.862662},
	issn = {0278-0062},
	journal = {IEEE Transactions on Medical Imaging},
	month = {feb},
	number = {2},
	pages = {218--228},
	title = {{Parallelized Bayesian inversion for three-dimensional dental X-ray imaging}},
	url = {http://ieeexplore.ieee.org/document/1583768/},
	volume = {25},
	year = {2006},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/1583768/},
	bdsk-url-2 = {https://doi.org/10.1109/TMI.2005.862662}}

@inproceedings{goodfellow2014generative,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Advances in neural information processing systems},
	pages = {2672--2680},
	title = {Generative adversarial nets},
	year = {2014}}

@article{Calvetti2008,
	author = {Calvetti, Daniela and Somersalo, Erkki},
	doi = {10.1088/0266-5611/24/3/034013},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {jun},
	number = {3},
	pages = {034013},
	publisher = {IOP Publishing},
	title = {{Hypermodels in the Bayesian imaging framework}},
	url = {http://stacks.iop.org/0266-5611/24/i=3/a=034013?key=crossref.10e6728aeff596e7626030876ce5e2a2},
	volume = {24},
	year = {2008},
	bdsk-url-1 = {http://stacks.iop.org/0266-5611/24/i=3/a=034013?key=crossref.10e6728aeff596e7626030876ce5e2a2},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/24/3/034013}}

@inproceedings{Bui-Thanh2012,
	author = {Bui-Thanh, Tan and Burstedde, Carsten and Ghattas, Omar and Martin, James and Stadler, Georg and Wilcox, Lucas C.},
	booktitle = {2012 International Conference for High Performance Computing, Networking, Storage and Analysis},
	doi = {10.1109/SC.2012.56},
	isbn = {978-1-4673-0805-2},
	month = {nov},
	pages = {1--11},
	publisher = {IEEE},
	title = {{Extreme-scale UQ for Bayesian inverse problems governed by PDEs}},
	url = {http://ieeexplore.ieee.org/document/6468442/},
	year = {2012},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/6468442/},
	bdsk-url-2 = {https://doi.org/10.1109/SC.2012.56}}

@article{Marzouk2009,
	abstract = {We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen--Lo{\`{e}}ve expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.},
	author = {Marzouk, Youssef M. and Najm, Habib N.},
	doi = {10.1016/J.JCP.2008.11.024},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	month = {apr},
	number = {6},
	pages = {1862--1902},
	publisher = {Academic Press},
	title = {{Dimensionality reduction and polynomial chaos acceleration of Bayesian inference in inverse problems}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999108006062},
	volume = {228},
	year = {2009},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999108006062},
	bdsk-url-2 = {https://doi.org/10.1016/J.JCP.2008.11.024}}

@article{Stuart2010,
	abstract = {The subject of inverse problems in differential equations is of enormous practical importance, and has also generated substantial mathematical and computational innovation. Typically some form of regularization is required to ameliorate ill-posed behaviour. In this article we review the Bayesian approach to regularization, developing a function space viewpoint on the subject. This approach allows for a full characterization of all possible solutions, and their relative probabilities, whilst simultaneously forcing significant modelling issues to be addressed in a clear and precise fashion. Although expensive to implement, this approach is starting to lie within the range of the available computational resources in many application areas. It also allows for the quantification of uncertainty and risk, something which is increasingly demanded by these applications. Furthermore, the approach is conceptually important for the understanding of simpler, computationally expedient approaches to inverse problems. We demonstrate that, when formulated in a Bayesian fashion, a wide range of inverse problems share a common mathematical framework, and we highlight a theory of well-posedness which stems from this. The well-posedness theory provides the basis for a number of stability and approximation results which we describe. We also review a range of algorithmic approaches which are used when adopting the Bayesian approach to inverse problems. These include MCMC methods, filtering and the variational approach.},
	author = {Stuart, A M},
	doi = {10.1017/S0962492910000061},
	journal = {Acta Numerica},
	pages = {451--559},
	publisher = {Cambridge University Press},
	title = {{Inverse problems: A Bayesian perspective}},
	url = {http://journals.cambridge.org/abstract{\_}S0962492910000061},
	volume = {19},
	year = {2010},
	bdsk-url-1 = {http://journals.cambridge.org/abstract%7B%5C_%7DS0962492910000061},
	bdsk-url-2 = {https://doi.org/10.1017/S0962492910000061}}

@article{Fahrmeir2001,
	author = {Fahrmeir, Ludwig and Lang, Stefan},
	doi = {10.1111/1467-9876.00229},
	issn = {0035-9254},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	keywords = {Generalized semiparametric mixed models,Markov chain Monte Carlo sampling,Random effects,Spatial and spatiotemporal data,State space and Markov random field models,Varying coefficients},
	month = {jan},
	number = {2},
	pages = {201--220},
	publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
	title = {{Bayesian inference for generalized additive mixed models based on Markov random field priors}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00229},
	volume = {50},
	year = {2001},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00229},
	bdsk-url-2 = {https://doi.org/10.1111/1467-9876.00229}}

@inproceedings{gulrajani2017improved,
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
	booktitle = {Advances in neural information processing systems},
	pages = {5767--5777},
	title = {Improved training of wasserstein gans},
	year = {2017}}

@article{Jin2017,
	author = {Jin, Kyong Hwan and McCann, Michael T. and Froustey, Emmanuel and Unser, Michael},
	doi = {10.1109/TIP.2017.2713099},
	issn = {1057-7149},
	journal = {IEEE Transactions on Image Processing},
	month = {sep},
	number = {9},
	pages = {4509--4522},
	title = {{Deep Convolutional Neural Network for Inverse Problems in Imaging}},
	url = {http://ieeexplore.ieee.org/document/7949028/},
	volume = {26},
	year = {2017},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/7949028/},
	bdsk-url-2 = {https://doi.org/10.1109/TIP.2017.2713099}}

@article{Adler2017,
	author = {Adler, Jonas and {\"{O}}ktem, Ozan},
	doi = {10.1088/1361-6420/aa9581},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {dec},
	number = {12},
	pages = {124007},
	publisher = {IOP Publishing},
	title = {{Solving ill-posed inverse problems using iterative deep neural networks}},
	url = {http://stacks.iop.org/0266-5611/33/i=12/a=124007?key=crossref.65c4fa88a47e07d4789aa10592f2090c},
	volume = {33},
	year = {2017},
	bdsk-url-1 = {http://stacks.iop.org/0266-5611/33/i=12/a=124007?key=crossref.65c4fa88a47e07d4789aa10592f2090c},
	bdsk-url-2 = {https://doi.org/10.1088/1361-6420/aa9581}}

@article{Andrieu2008,
	abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
	author = {Andrieu, Christophe and Thoms, Johannes},
	doi = {10.1007/s11222-008-9110-y},
	issn = {09603174},
	journal = {Statistics and Computing},
	keywords = {Adaptive MCMC,Controlled Markov chain,MCMC,Stochastic approximation},
	month = {dec},
	number = {4},
	pages = {343--373},
	title = {{A tutorial on adaptive MCMC}},
	volume = {18},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-008-9110-y}}

@incollection{Snieder1999inverse,
	abstract = {An important aspect of the physical sciences is to make inferences about physical parameters from data. In general, the laws of physics provide the means for computing the data values given a model. This is called the ``forward problem'', see figure 1. In the inverse problem, the aim is to reconstruct the model from a set of measurements. In the ideal case, an exact theory exists that prescribes how the data should be transformed in order to reproduce the model. For some selected examples such a theory exists assuming that the required infinite and noise-free data sets would be available. A quantum mechanical potential in one spatial dimension can be reconstructed when the reflection coefficient is known for all energies [Marchenko, 1955; Burridge, 1980]. This technique can be generalized for the reconstruction of a quantum mechanical potential in three dimensions [Newton, 1989], but in that case a redundant data set is required for reasons that are not well understood. The mass-density in a one-dimensional string can be constructed from the measurements of all eigenfrequencies of that string [Borg, 1946], but due to the symmetry of this problem only the even part of the mass-density can be determined. If the seismic velocity},
	author = {Snieder, R. and Trampert, J.},
	doi = {10.1007/978-3-7091-2486-4_3},
	pages = {119--190},
	publisher = {Springer, Vienna},
	title = {{Inverse Problems in Geophysics}},
	url = {https://link.springer.com/chapter/10.1007/978-3-7091-2486-4{\_}3},
	year = {1999},
	bdsk-url-1 = {https://link.springer.com/chapter/10.1007/978-3-7091-2486-4%7B%5C_%7D3},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-7091-2486-4_3}}

@article{Huang2005inverse,
	abstract = {This paper reviews various kinds of inverse problems in atmospheric science and oceanography, and introduces powerful methods to treat these problems--variational data assimilation (VAR) and improved discrepancy principle, and discusses some essential difficulties in VAR. Due to the ill--posedness of these problems, the regularization method is also applied, i.e., additional terms are added to the cost functional as a stabilized functional with physical meaning. Inversions of four specific problems, such as the inversion of one-dimensional sea temperature model, the inversion of parameters in an ENSO cycle model, the inversion of wind field with single-Doppler data, and the inversion of satellite remote sensing, indicate that, adoption of the regularization method in VAR will overcome the ill-posedness, constrain calculational oscillations in iteration, and speed up convergence of solutions.},
	author = {Huang, Sixun and Xiang, Jie and Du, Huadong and Cao, Xiaoqun},
	doi = {10.1088/1742-6596/12/1/005},
	journal = {Journal of Physics: Conference Series},
	month = {jan},
	pages = {45--57},
	publisher = {{\{}IOP{\}} Publishing},
	title = {{Inverse problems in atmospheric science and their application}},
	url = {https://doi.org/10.1088/1742-6596/12/1/005},
	volume = {12},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1088/1742-6596/12/1/005}}

@article{Yang2020pigan,
	abstract = {We developed a new class of physics-informed generative adversarial networks (PIGANs) to solve forward, inverse, and mixed stochastic problems in a unified manner based on a limited number of scattered measurements. Unlike standard GANs relying solely on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even if there is a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and the generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. Here again, we assumed data collected from simultaneous reads at a limited number of sensors for the multiple stochastic processes. Three generators were used for the PI-GANs: Two of them were feed forward deep neural networks (DNNs), while the other one was the neural network induced by the SDE. For the case where we have one group of data, we employed one feed forward DNN as the discriminator, while for the case of multiple groups of data we employed multiple discriminators in PI-GANs. We solved forward, inverse, and mixed problems without changing the framework of PI-GANs, obtaining both the means and the standard deviations of the stochastic solution and the diffusion coefficient in good agreement with benchmarks. In this work, we have demonstrated the effectiveness of PI-GANs in solving SDEs for about 120 dimensions. In principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.},
	archiveprefix = {arXiv},
	arxivid = {1811.02033},
	author = {Yang, Liu and Zhang, Dongkun and Karniadakis, George E.M.},
	doi = {10.1137/18M1225409},
	eprint = {1811.02033},
	issn = {10957197},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {Elliptic stochastic problems,High-dimensional problems,Inverse problems,Multiplayer GANs,WGAN-GP},
	month = {feb},
	number = {1},
	pages = {A292--A317},
	publisher = {Society for Industrial and Applied Mathematics Publications},
	title = {{Physics-informed generative adversarial networks for stochastic differential equations}},
	volume = {42},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1137/18M1225409}}

@book{Brooks2011,
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory. The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology. The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
	author = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao Li},
	booktitle = {Handbook of Markov Chain Monte Carlo},
	doi = {10.1201/b10905},
	isbn = {9781420079425},
	month = {may},
	pages = {1--592},
	publisher = {CRC Press},
	title = {{Handbook of Markov Chain Monte Carlo}},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1201/b10905}}

@article{yang2012climate,
	author = {Yang, B and Qian, Y and Lin, G and Leung, R and Zhang, Y},
	doi = {10.5194/acp-12-2409-2012},
	journal = {Atmospheric Chemistry and Physics},
	number = {5},
	pages = {2409--2427},
	title = {{Some issues in uncertainty quantification and parameter tuning: a case study of convective parameterization scheme in the WRF regional climate model}},
	url = {https://acp.copernicus.org/articles/12/2409/2012/},
	volume = {12},
	year = {2012},
	bdsk-url-1 = {https://acp.copernicus.org/articles/12/2409/2012/},
	bdsk-url-2 = {https://doi.org/10.5194/acp-12-2409-2012}}

@article{devito05a,
	author = {Ernesto De Vito and Lorenzo Rosasco and Andrea Caponnetto and Umberto De Giovannini and Francesca Odone},
	journal = {Journal of Machine Learning Research},
	number = {30},
	pages = {883-904},
	title = {Learning from Examples as an Inverse Problem},
	url = {http://jmlr.org/papers/v6/devito05a.html},
	volume = {6},
	year = {2005},
	bdsk-url-1 = {http://jmlr.org/papers/v6/devito05a.html}}

@article{Alnaes2015fenics,
	abstract = {The FEniCS Project is a collaborative project for the development of innovative concepts and tools for automated scientific computing, with a particular focus on the solution of differential equations by finite element methods. The FEniCS Projects software consists of a collection of interoperable software components, including DOLFIN, FFC, FIAT, Instant, UFC, UFL, and mshr. This note describes the new features and changes introduced in the release of FEniCS version 1.5.},
	author = {Alnaes, Martin S and Blechta, Jan and Hake, Johan and Johansson, August and Kehlet, Benjamin and Logg, Anders and Richardson, Chris and Ring, Johannes and Rognes, Marie E and Wells, Garth N},
	doi = {10.11588/ans.2015.100.20553},
	issn = {2197-8263},
	journal = {The FEniCS Project Version 1.5},
	keywords = {Component-based software engineering,Computer science,Differential equation,Finite element method,Interoperability,Software,Software engineering},
	month = {dec},
	number = {100},
	pages = {9--23},
	title = {{The FEniCS Project Version 1.5}},
	url = {http://fenicsproject.org.},
	volume = {3},
	year = {2015},
	bdsk-url-1 = {http://fenicsproject.org.},
	bdsk-url-2 = {https://doi.org/10.11588/ans.2015.100.20553}}

@techreport{Chang,
	abstract = {While deep learning methods have achieved state-of-the-art performance in many challenging inverse problems like image inpainting and super-resolution, they invariably involve problem-specific training of the networks. Under this approach, different problems require different networks. In scenarios where we need to solve a wide variety of problems, e.g., on a mobile camera, it is inefficient and costly to use these specially-trained networks. On the other hand, traditional methods using signal priors can be used in all linear inverse problems but often have worse performance on challenging tasks. In this work, we provide a middle ground between the two kinds of methods-we propose a general framework to train a single deep neural network that solves arbitrary linear inverse problems. The proposed network acts as a proximal operator for an optimization algorithm and projects non-image signals onto the set of natural images defined by the decision boundary of a classifier. In our experiments, the proposed framework demonstrates superior performance over traditional methods using a wavelet sparsity prior and achieves comparable performance of specially-trained networks on tasks including compressive sensing and pixel-wise inpainting.},
	archiveprefix = {arXiv},
	arxivid = {1703.09912v1},
	author = {Chang, J H Rick and Li, Chun-Liang and Barnab{\'{a}}s, Barnab´ and Oczos, P ´ and Kumar, B V K Vijaya and Sankaranarayanan, Aswin C},
	eprint = {1703.09912v1},
	keywords = {Adversarial learning,Image processing,Index Terms-Linear inverse problems,Proximal operator !},
	title = {{One Network to Solve Them All-Solving Linear Inverse Problems using Deep Projection Models}},
	url = {https://arxiv.org/pdf/1703.09912.pdf},
	bdsk-url-1 = {https://arxiv.org/pdf/1703.09912.pdf}}

@article{Yang2018,
	author = {Yang, Qingsong and Yan, Pingkun and Zhang, Yanbo and Yu, Hengyong and Shi, Yongyi and Mou, Xuanqin and Kalra, Mannudeep K. and Zhang, Yi and Sun, Ling and Wang, Ge},
	doi = {10.1109/TMI.2018.2827462},
	issn = {0278-0062},
	journal = {IEEE Transactions on Medical Imaging},
	month = {jun},
	number = {6},
	pages = {1348--1357},
	title = {{Low-Dose CT Image Denoising Using a Generative Adversarial Network With Wasserstein Distance and Perceptual Loss}},
	url = {https://ieeexplore.ieee.org/document/8340157/},
	volume = {37},
	year = {2018},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8340157/},
	bdsk-url-2 = {https://doi.org/10.1109/TMI.2018.2827462}}

@techreport{Lediga,
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper con-volutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	author = {Ledig, Christian and Theis, Lucas and Husz{\'{a}}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and {Shi Twitter}, Wenzhe},
	title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
	url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/papers/Ledig{\_}Photo-Realistic{\_}Single{\_}Image{\_}CVPR{\_}2017{\_}paper.pdf},
	bdsk-url-1 = {http://openaccess.thecvf.com/content%7B%5C_%7Dcvpr%7B%5C_%7D2017/papers/Ledig%7B%5C_%7DPhoto-Realistic%7B%5C_%7DSingle%7B%5C_%7DImage%7B%5C_%7DCVPR%7B%5C_%7D2017%7B%5C_%7Dpaper.pdf}}

@article{Vauhkonen1997,
	author = {Vauhkonen, M and Kaipio, J P and Somersalo, E and Karjalainen, P A},
	doi = {10.1088/0266-5611/13/2/020},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {apr},
	number = {2},
	pages = {523--530},
	publisher = {IOP Publishing},
	title = {{Electrical impedance tomography with basis constraints}},
	url = {http://stacks.iop.org/0266-5611/13/i=2/a=020?key=crossref.46559bf45aab26a8302acc14e8db4c89},
	volume = {13},
	year = {1997},
	bdsk-url-1 = {http://stacks.iop.org/0266-5611/13/i=2/a=020?key=crossref.46559bf45aab26a8302acc14e8db4c89},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/13/2/020}}

@misc{nowozin2016fgan,
	archiveprefix = {arXiv},
	author = {Sebastian Nowozin and Botond Cseke and Ryota Tomioka},
	eprint = {1606.00709},
	primaryclass = {stat.ML},
	title = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},
	year = {2016}}

@misc{poole2016improved,
	archiveprefix = {arXiv},
	author = {Ben Poole and Alexander A. Alemi and Jascha Sohl-Dickstein and Anelia Angelova},
	eprint = {1612.02780},
	primaryclass = {cs.LG},
	title = {Improved generator objectives for GANs},
	year = {2016}}

@article{Cannon1986,
	author = {Cannon, J R and Esteva, S P},
	doi = {10.1088/0266-5611/2/4/007},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {nov},
	number = {4},
	pages = {395--403},
	title = {{An inverse problem for the heat equation}},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/2/4/007},
	volume = {2},
	year = {1986},
	bdsk-url-1 = {https://iopscience.iop.org/article/10.1088/0266-5611/2/4/007},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/2/4/007}}

@article{Cannon1998structural,
	abstract = {The identification of an unknown state-dependent source term in a reaction-diffusion equation is considered. Integral identities are derived which relate changes in the source term to corresponding changes in the measured output. The identities are used to show that the measured boundary output determines the source term uniquely in an appropriate function class and to show that a source term that minimizes an output least squares functional based on this measured output must also solve the inverse problem. The set of outputs generated by polygonal source functions is shown to be dense in the set of all admissible outputs. Results from some numerical experiments are discussed.},
	author = {Cannon, J. R. and Duchateau, Paul},
	doi = {10.1088/0266-5611/14/3/010},
	issn = {02665611},
	journal = {Inverse Problems},
	month = {jun},
	number = {3},
	pages = {535--551},
	publisher = {IOP Publishing},
	title = {{Structural identification of an unknown source term in a heat equation}},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/14/3/010 https://iopscience.iop.org/article/10.1088/0266-5611/14/3/010/meta},
	volume = {14},
	year = {1998},
	bdsk-url-1 = {https://iopscience.iop.org/article/10.1088/0266-5611/14/3/010%20https://iopscience.iop.org/article/10.1088/0266-5611/14/3/010/meta},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/14/3/010}}

@article{Cheng2010,
	abstract = {We consider an inverse heat source problem of determining the heat source term from the final temperature history of a cylinder. This problem is ill-posed. A simplified Tikhonov regularization method is applied to formulate regularized solution, which is stably convergent to the exact one with a logarithmic type error estimate. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
	author = {Cheng, Wei and Zhao, Ling Ling and Fu, Chu Li},
	doi = {10.1016/j.camwa.2009.08.038},
	file = {:C$\backslash$:/Users/Harikrishna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Zhao, Fu - 2010 - Source term identification for an axisymmetric inverse heat conduction problem.pdf:pdf},
	issn = {08981221},
	journal = {Computers and Mathematics with Applications},
	keywords = {Error estimate,Identification,Ill-posed problem,Inverse source problem,Regularization},
	month = {jan},
	number = {1},
	pages = {142--148},
	publisher = {Pergamon},
	title = {{Source term identification for an axisymmetric inverse heat conduction problem}},
	volume = {59},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1016/j.camwa.2009.08.038}}

@article{Su2001,
	abstract = {The radial and circumferential (azimuthal) transient dependence of the strength of a volumetric heat source in a cylindrical rod is estimated with Alifanov's iterative regularization method. This inverse problem is solved as an optimization problem in which a squared residue functional is minimized with the conjugate gradient method. A sensitivity problem is used in the determination of the step size in the direction of descent, while an adjoint problem is solved to determine the gradient. In order to examine the accuracy of estimations, two test cases are considered, one with a radial and timewise dependence and the second with radial, azimuthal as well as timewise dependence. The effects of number of sensors and measurement errors are investigated. {\textcopyright} 2001 Elsevier Science Inc. All rights reserved.},
	author = {Su, Jian and {Silva Neto}, Ant{\^{o}}nio J.},
	doi = {10.1016/S0307-904X(01)00018-X},
	file = {:C$\backslash$:/Users/Harikrishna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su, Silva Neto - 2001 - Two-dimensional inverse heat conduction problem of source strength estimation in cylindrical rods.pdf:pdf},
	issn = {0307904X},
	journal = {Applied Mathematical Modelling},
	keywords = {Adjoint problem,Conjugate gradient method,Heat conduction,Inverse problem},
	month = {oct},
	number = {10},
	pages = {861--872},
	publisher = {Elsevier},
	title = {{Two-dimensional inverse heat conduction problem of source strength estimation in cylindrical rods}},
	volume = {25},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1016/S0307-904X(01)00018-X}}

@article{Calvetti2000,
	abstract = {Discretization of linear inverse problems generally gives rise to very ill-conditioned linear systems of algebraic equations. Typically, the linear systems obtained have to be regularized to make the computation of a meaningful approximate solution possible. Tikhonov regularization is one of the most popular regularization methods. A regularization parameter specifies the amount of regularization and, in general, an appropriate value of this parameter is not known a priori. We review available iterative methods, and present new ones, for the determination of a suitable value of the regularization parameter by the L-curve criterion and the solution of regularized systems of algebraic equations. {\textcopyright} 2000 Elsevier Science B.V.},
	author = {Calvetti, D. and Morigi, S. and Reichel, L. and Sgallari, F.},
	doi = {10.1016/S0377-0427(00)00414-3},
	issn = {03770427},
	journal = {Journal of Computational and Applied Mathematics},
	keywords = {Gauss quadrature,Ill-posed problem,L-curve criterion,Regularization},
	month = {nov},
	number = {1-2},
	pages = {423--446},
	publisher = {Elsevier},
	title = {{Tikhonov regularization and the L-curve for large discrete ill-posed problems}},
	volume = {123},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1016/S0377-0427(00)00414-3}}

@article{Calvetti2005,
	author = {Calvetti, Daniela and Somersalo, Erkki},
	doi = {10.1088/0266-5611/21/4/014},
	issn = {0266-5611},
	journal = {Inverse Problems},
	month = {aug},
	number = {4},
	pages = {1397--1418},
	publisher = {IOP Publishing},
	title = {{Priorconditioners for linear systems}},
	url = {http://stacks.iop.org/0266-5611/21/i=4/a=014?key=crossref.ab419ffb66111e3db21bf3d9fd3836f7},
	volume = {21},
	year = {2005},
	bdsk-url-1 = {http://stacks.iop.org/0266-5611/21/i=4/a=014?key=crossref.ab419ffb66111e3db21bf3d9fd3836f7},
	bdsk-url-2 = {https://doi.org/10.1088/0266-5611/21/4/014}}

@article{Lieberman2010,
	author = {Lieberman, Chad and Willcox, Karen and Ghattas, Omar},
	doi = {10.1137/090775622},
	issn = {1064-8275},
	journal = {SIAM Journal on Scientific Computing},
	month = {jan},
	number = {5},
	pages = {2523--2542},
	title = {{Parameter and State Model Reduction for Large-Scale Statistical Inverse Problems}},
	url = {http://epubs.siam.org/doi/10.1137/090775622},
	volume = {32},
	year = {2010},
	bdsk-url-1 = {http://epubs.siam.org/doi/10.1137/090775622},
	bdsk-url-2 = {https://doi.org/10.1137/090775622}}

@misc{Kupyn2018,
	author = {Kupyn, Orest and Budzan, Volodymyr and Mykhailych, Mykola and Mishkin, Dmytro and Matas, Ji{\v r}{\'{i}}},
	pages = {8183--8192},
	title = {{DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks}},
	url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Kupyn{\_}DeblurGAN{\_}Blind{\_}Motion{\_}CVPR{\_}2018{\_}paper.html},
	year = {2018},
	bdsk-url-1 = {http://openaccess.thecvf.com/content%7B%5C_%7Dcvpr%7B%5C_%7D2018/html/Kupyn%7B%5C_%7DDeblurGAN%7B%5C_%7DBlind%7B%5C_%7DMotion%7B%5C_%7DCVPR%7B%5C_%7D2018%7B%5C_%7Dpaper.html}}

@misc{tensorflow2015-whitepaper,
	author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	note = {Software available from tensorflow.org},
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	year = {2015},
	bdsk-url-1 = {https://www.tensorflow.org/}}

@article{Makhzani2015,
	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	archiveprefix = {arXiv},
	arxivid = {1511.05644},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	eprint = {1511.05644},
	month = {nov},
	title = {{Adversarial Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1511.05644}}

@article{Dumoulin2016,
	abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
	archiveprefix = {arXiv},
	arxivid = {1606.00704},
	author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
	eprint = {1606.00704},
	month = {jun},
	title = {{Adversarially Learned Inference}},
	url = {http://arxiv.org/abs/1606.00704},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1606.00704}}

@article{Mescheder2017,
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	archiveprefix = {arXiv},
	arxivid = {1701.04722},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	eprint = {1701.04722},
	month = {jan},
	title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1701.04722},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1701.04722}}

@article{Zhang2016,
	abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
	archiveprefix = {arXiv},
	arxivid = {1612.03242},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
	eprint = {1612.03242},
	month = {dec},
	title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1612.03242},
	year = {2016},
	bdsk-url-1 = {http://arxiv.org/abs/1612.03242}}

@article{Wang2017,
	abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
	archiveprefix = {arXiv},
	arxivid = {1711.11585},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	eprint = {1711.11585},
	month = {nov},
	title = {{High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs}},
	url = {http://arxiv.org/abs/1711.11585},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1711.11585}}

@article{Ma2017,
	abstract = {This paper proposes the novel Pose Guided Person Generation Network (PG{\$}{\^{}}2{\$}) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG{\$}{\^{}}2{\$} utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128{\$}\backslashtimes{\$}64 re-identification images and 256{\$}\backslashtimes{\$}256 fashion photos show that our model generates high-quality person images with convincing details.},
	archiveprefix = {arXiv},
	arxivid = {1705.09368},
	author = {Ma, Liqian and Jia, Xu and Sun, Qianru and Schiele, Bernt and Tuytelaars, Tinne and {Van Gool}, Luc},
	eprint = {1705.09368},
	month = {may},
	title = {{Pose Guided Person Image Generation}},
	url = {http://arxiv.org/abs/1705.09368},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1705.09368}}

@article{pix2pix2016,
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
	journal = {arxiv},
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	year = {2016}}

@inproceedings{CycleGAN2017,
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
	booktitle = {Computer Vision (ICCV), 2017 IEEE International Conference on},
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	year = {2017}}

@article{Anirudh2018,
	abstract = {Solving inverse problems continues to be a challenge in a wide array of applications ranging from deblurring, image inpainting, source separation etc. Most existing techniques solve such inverse problems by either explicitly or implicitly finding the inverse of the model. The former class of techniques require explicit knowledge of the measurement process which can be unrealistic, and rely on strong analytical regularizers to constrain the solution space, which often do not generalize well. The latter approaches have had remarkable success in part due to deep learning, but require a large collection of source-observation pairs, which can be prohibitively expensive. In this paper, we propose an unsupervised technique to solve inverse problems with generative adversarial networks (GANs). Using a pre-trained GAN in the space of source signals, we show that one can reliably recover solutions to under determined problems in a `blind' fashion, i.e., without knowledge of the measurement process. We solve this by making successive estimates on the model and the solution in an iterative fashion. We show promising results in three challenging applications -- blind source separation, image deblurring, and recovering an image from its edge map, and perform better than several baselines.},
	archiveprefix = {arXiv},
	arxivid = {1805.07281},
	author = {Anirudh, Rushil and Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Bremer, Timo},
	eprint = {1805.07281},
	month = {may},
	title = {{An Unsupervised Approach to Solving Inverse Problems using Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1805.07281},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1805.07281}}

@article{Kim2017,
	abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
	archiveprefix = {arXiv},
	arxivid = {1703.05192},
	author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
	eprint = {1703.05192},
	month = {mar},
	title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1703.05192},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1703.05192}}

@article{Gal2015,
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	archiveprefix = {arXiv},
	arxivid = {1506.02142},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	eprint = {1506.02142},
	month = {jun},
	title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
	url = {http://arxiv.org/abs/1506.02142},
	year = {2015},
	bdsk-url-1 = {http://arxiv.org/abs/1506.02142}}

@article{Brock2018,
	archiveprefix = {arXiv},
	arxivid = {1809.11096},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	eprint = {1809.11096},
	month = {sep},
	title = {{Large Scale GAN Training for High Fidelity Natural Image Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1809.11096}}

@article{Karras2017,
	archiveprefix = {arXiv},
	arxivid = {1710.10196},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	eprint = {1710.10196},
	file = {:C$\backslash$:/Users/Harikrishna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:pdf},
	month = {oct},
	title = {{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1710.10196}}

@article{fedus2018maskgan,
	author = {Fedus, William and Goodfellow, Ian and Dai, Andrew M},
	journal = {arXiv preprint arXiv:1801.07736},
	title = {MaskGAN: better text generation via filling in the\_},
	year = {2018}}

@inproceedings{tulyakov2018mocogan,
	author = {Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {1526--1535},
	title = {Mocogan: Decomposing motion and content for video generation},
	year = {2018}}

@inproceedings{lunz2018adversarial,
	author = {Lunz, Sebastian and {\"O}ktem, Ozan and Sch{\"o}nlieb, Carola-Bibiane},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {8507--8516},
	title = {Adversarial regularizers in inverse problems},
	year = {2018}}

@inproceedings{bora2017compressed,
	author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	organization = {JMLR. org},
	pages = {537--546},
	title = {Compressed sensing using generative models},
	year = {2017}}

@article{bora2018ambientgan,
	author = {Bora, Ashish and Price, Eric and Dimakis, Alexandros G},
	journal = {ICLR},
	pages = {5},
	title = {AmbientGAN: Generative models from lossy measurements.},
	volume = {2},
	year = {2018}}

@inproceedings{kabkab2018task,
	author = {Kabkab, Maya and Samangouei, Pouya and Chellappa, Rama},
	booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
	title = {Task-aware compressed sensing with generative adversarial networks},
	year = {2018}}


@misc{adler2018deep,
  doi = {10.48550/ARXIV.1811.05910},
  
  howpublished = "\url{https://arxiv.org/abs/1811.05910}",
  
  author = {Adler, Jonas and Öktem, Ozan},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Deep Bayesian Inversion},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{wu2019deep,
	author = {Wu, Yan and Rosca, Mihaela and Lillicrap, Timothy},
	journal = {arXiv preprint arXiv:1905.06723},
	title = {Deep Compressed Sensing},
	year = {2019}}

@inproceedings{shah2018solving,
	author = {Shah, Viraj and Hegde, Chinmay},
	booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	organization = {IEEE},
	pages = {4609--4613},
	title = {Solving linear inverse problems using gan priors: An algorithm with provable guarantees},
	year = {2018}}

@phdthesis{gal2016uncertainty,
	author = {Gal, Yarin},
	school = {PhD thesis, University of Cambridge},
	title = {Uncertainty in deep learning},
	year = {2016}}

@article{Hastings1970,
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	author = {Hastings, W. K.},
	doi = {10.2307/2334940},
	issn = {00063444},
	journal = {Biometrika},
	month = {apr},
	number = {1},
	pages = {97},
	publisher = {Oxford University PressBiometrika Trust},
	title = {{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}},
	url = {https://www.jstor.org/stable/2334940?origin=crossref},
	volume = {57},
	year = {1970},
	bdsk-url-1 = {https://www.jstor.org/stable/2334940?origin=crossref},
	bdsk-url-2 = {https://doi.org/10.2307/2334940}}

@article{Metropolis1953,
	abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The metho...},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	doi = {10.1063/1.1699114},
	issn = {0021-9606},
	journal = {The Journal of Chemical Physics},
	month = {jun},
	number = {6},
	pages = {1087--1092},
	publisher = {American Institute of Physics},
	title = {{Equation of State Calculations by Fast Computing Machines}},
	url = {http://aip.scitation.org/doi/10.1063/1.1699114},
	volume = {21},
	year = {1953},
	bdsk-url-1 = {http://aip.scitation.org/doi/10.1063/1.1699114},
	bdsk-url-2 = {https://doi.org/10.1063/1.1699114}}

@article{Atchade2006,
	author = {Atchad{\'{e}}, Yves F.},
	doi = {10.1007/s11009-006-8550-0},
	issn = {1387-5841},
	journal = {Methodology and Computing in Applied Probability},
	month = {jun},
	number = {2},
	pages = {235--254},
	publisher = {Kluwer Academic Publishers},
	title = {{An Adaptive Version for the Metropolis Adjusted Langevin Algorithm with a Truncated Drift}},
	url = {http://link.springer.com/10.1007/s11009-006-8550-0},
	volume = {8},
	year = {2006},
	bdsk-url-1 = {http://link.springer.com/10.1007/s11009-006-8550-0},
	bdsk-url-2 = {https://doi.org/10.1007/s11009-006-8550-0}}

@techreport{Brooks2012,
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor-a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and shortcut methods that prevent useless trajectories from taking much computation time.},
	archiveprefix = {arXiv},
	arxivid = {1206.1901v1},
	author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li and Neal, Radford M},
	eprint = {1206.1901v1},
	keywords = {()},
	title = {{MCMC using Hamiltonian dynamics}},
	url = {https://arxiv.org/pdf/1206.1901.pdf},
	year = {2012},
	bdsk-url-1 = {https://arxiv.org/pdf/1206.1901.pdf}}

@techreport{Hoffman2014,
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
	author = {Hoffman, Matthew D and Gelman, Andrew},
	booktitle = {Journal of Machine Learning Research},
	keywords = {Bayesian inference,Hamiltonian Monte Carlo,Markov chain Monte Carlo,adaptive Monte Carlo,dual averaging},
	pages = {1351--1381},
	title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
	url = {http://mcmc-jags.sourceforge.net},
	volume = {15},
	year = {2014},
	bdsk-url-1 = {http://mcmc-jags.sourceforge.net}}

@article{han2001markov,
	author = {Han, Cong and Carlin, Bradley P},
	journal = {Journal of the American Statistical Association},
	number = {455},
	pages = {1122--1132},
	publisher = {Taylor \& Francis},
	title = {Markov chain Monte Carlo methods for computing Bayes factors: A comparative review},
	volume = {96},
	year = {2001}}

@article{lecun2010mnist,
	author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	title = {MNIST handwritten digit database},
	volume = {2},
	year = {2010}}

@article{Dillon2017,
	abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
	archiveprefix = {arXiv},
	arxivid = {1711.10604},
	author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
	eprint = {1711.10604},
	month = {nov},
	title = {{TensorFlow Distributions}},
	url = {http://arxiv.org/abs/1711.10604},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1711.10604}}

@article{Petra2012inexact,
	abstract = {We propose an infinite-dimensional adjoint-based inexact Gauss-Newton method for the solution of inverse problems governed by Stokes models of ice sheet flow with nonlinear rheology and sliding law. The method is applied to infer the basal sliding coefficient and the rheological exponent parameter fields from surface velocities. The inverse problem is formulated as a nonlinear least-squares optimization problem whose cost functional is the misfit between surface velocity observations and model predictions. A Tikhonov regularization term is added to the cost functional to render the problem well-posed and account for observational error. Our findings show that the inexact Newton method is significantly more efficient than the nonlinear conjugate gradient method and that the number of Stokes solutions required to solve the inverse problem is insensitive to the number of inversion parameters. The results also show that the reconstructions of the basal sliding coefficient converge to the exact sliding coefficient as the observation error (here, the noise added to synthetic observations) decreases, and that a nonlinear rheology makes the reconstruction of the basal sliding coefficient more difficult. For the inversion of the rheology exponent field, we find that horizontally constant or smoothly varying parameter fields can be reconstructed satisfactorily from noisy observations.},
	author = {Petra, Noemi and Zhu, Hongyu and Stadler, Georg and Hughes, Thomas J.R. and Ghattas, Omar},
	doi = {10.3189/2012JoG11J182},
	file = {:C$\backslash$:/Users/Harikrishna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petra et al. - 2012 - An inexact Gauss-Newton method for inversion of basal sliding and rheology parameters in a nonlinear Stokes ice sh.pdf:pdf},
	issn = {00221430},
	journal = {Journal of Glaciology},
	month = {sep},
	number = {211},
	pages = {889--903},
	publisher = {Cambridge University Press},
	title = {{An inexact Gauss-Newton method for inversion of basal sliding and rheology parameters in a nonlinear Stokes ice sheet model}},
	url = {https://www.cambridge.org/core.},
	volume = {58},
	year = {2012},
	bdsk-url-1 = {https://www.cambridge.org/core.},
	bdsk-url-2 = {https://doi.org/10.3189/2012JoG11J182}}

@article{parno2018transport,
	author = {Parno, Matthew D and Marzouk, Youssef M},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	number = {2},
	pages = {645--682},
	publisher = {SIAM},
	title = {Transport Map Accelerated Markov Chain Monte Carlo},
	volume = {6},
	year = {2018}}

@article{patel2019circumventing,
	author = {Patel, Dhruv and Tibrewala, Raghav and Vega, Adriana and Dong, Li and Hugenberg, Nicholas and Oberai, Assad A},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	pages = {448--466},
	publisher = {Elsevier},
	title = {Circumventing the solution of inverse problems in mechanics through deep learning: Application to elasticity imaging},
	volume = {353},
	year = {2019}}


@article{patel2021,
	author = {Patel, Dhruv V. and Oberai, Assad A.},
	doi = {10.1137/20M1354210},
	eprint = {https://doi.org/10.1137/20M1354210},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	number = {3},
	pages = {1314-1343},
	title = {GAN-Based Priors for Quantifying Uncertainty in Supervised Learning},
	url = {https://doi.org/10.1137/20M1354210},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1137/20M1354210}}


@phdthesis{toft_1996,
	author = {Toft, Peter Aundal},
	school = {Technical University of Denmark},
	title = {The Radon Transform - Theory and Implementation},
	year = {1996}}

@incollection{barbone2010review,
	author = {Barbone, Paul E and Oberai, Assad A},
	booktitle = {Computational Modeling in Biomechanics},
	pages = {375--408},
	publisher = {Springer},
	title = {A review of the mathematical and computational foundations of biomechanical imaging},
	year = {2010}}

@article{pavan2010nonlinear,
	author = {Pavan, Theo Z and Madsen, Ernest L and Frank, Gary R and Carneiro, Antonio Adilton O and Hall, Timothy J},
	journal = {Physics in Medicine \& Biology},
	number = {9},
	pages = {2679},
	publisher = {IOP Publishing},
	title = {Nonlinear elastic behavior of phantom materials for elastography},
	volume = {55},
	year = {2010}}

@article{Iglesias2013EvaluationOG,
	author = {Iglesias, M and Law, K and Stuart, A},
	journal = {Computational Geosciences},
	pages = {851--885},
	title = {{Evaluation of Gaussian approximations for data assimilation in reservoir models}},
	volume = {17},
	year = {2013}}

@article{Kaipio2011,
	author = {Kaipio, Jari P and Fox, Colin},
	doi = {10.1080/01457632.2011.525137},
	journal = {Heat Transfer Engineering},
	number = {9},
	pages = {718--753},
	publisher = {Taylor {\&} Francis},
	title = {{The Bayesian Framework for Inverse Problems in Heat Transfer}},
	url = {https://doi.org/10.1080/01457632.2011.525137},
	volume = {32},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1080/01457632.2011.525137}}

@book{kirsch2021introduction,
	author = {Kirsch, Andreas},
	publisher = {Springer Nature},
	title = {An introduction to the mathematical theory of inverse problems},
	volume = {120},
	year = {2021}}

@book{isakov2006inverse,
	author = {Isakov, Victor},
	publisher = {Springer},
	title = {Inverse problems for partial differential equations},
	volume = {127},
	year = {2006}}

@book{kaipio2006statistical,
	author = {Kaipio, Jari and Somersalo, Erkki},
	publisher = {Springer Science \& Business Media},
	title = {Statistical and computational inverse problems},
	volume = {160},
	year = {2006}}

@article{yang2019adversarial,
	author = {Yang, Yibo and Perdikaris, Paris},
	journal = {Journal of Computational Physics},
	pages = {136--152},
	publisher = {Elsevier},
	title = {Adversarial uncertainty quantification in physics-informed neural networks},
	volume = {394},
	year = {2019}}

@article{doyley2014elastography,
	author = {Doyley, Marvin M and Parker, Kevin J},
	journal = {Ultrasound clinics},
	number = {1},
	pages = {1},
	publisher = {NIH Public Access},
	title = {Elastography: general principles and clincial applications},
	volume = {9},
	year = {2014}}

@book{natterer2001,
	author = {Natterer, F.},
	doi = {10.1137/1.9780898719284},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898719284},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {The Mathematics of Computerized Tomography},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898719284},
	year = {2001},
	bdsk-url-1 = {https://epubs.siam.org/doi/abs/10.1137/1.9780898719284},
	bdsk-url-2 = {https://doi.org/10.1137/1.9780898719284}}


@article{wang2021understanding,
  title={Understanding and mitigating gradient flow pathologies in physics-informed neural networks},
  author={Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3055--A3081},
  year={2021},
  publisher={SIAM}
}


@misc{mcclenny2020self,
  doi = {10.48550/ARXIV.2009.04544},
  
  howpublished="\url{https://arxiv.org/abs/2009.04544}",
  
  author = {McClenny, Levi and Braga-Neto, Ulisses},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bischof2021multi,
  doi = {10.13140/RG.2.2.20057.24169},
  
  howpublished = "\url{http://rgdoi.net/10.13140/RG.2.2.20057.24169}",
  
  author = {Bischof, Rafael and Kraus, Michael},
  
  language = {en},
  
  title = {Multi-Objective Loss Balancing for Physics-Informed Deep Learning},
  
  publisher = {Unpublished},
  
  year = {2021}
}



@INPROCEEDINGS{yang2018_sem,
  author={Yang, Maoke and Yu, Kun and Zhang, Chi and Li, Zhiwei and Yang, Kuiyuan},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={DenseASPP for Semantic Segmentation in Street Scenes}, 
  year={2018},
  volume={},
  number={},
  pages={3684-3692},
  doi={10.1109/CVPR.2018.00388}}

  @misc{mirza2014,
  doi = {10.48550/ARXIV.1411.1784},
  
  howpublished="\url{https://arxiv.org/abs/1411.1784}",
  
  author = {Mirza, Mehdi and Osindero, Simon},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Conditional Generative Adversarial Nets},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{ray2022,
  doi = {10.48550/ARXIV.2202.07773},
  
  howpublished="\url{https://arxiv.org/abs/2202.07773}",
  
  author = {Ray, Deep and Ramaswamy, Harisankar and Patel, Dhruv V. and Oberai, Assad A.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, 62F15, 68T07, 65M32},
  
  title = {The efficacy and generalizability of conditional GANs for posterior inference in physics-based inverse problems},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}